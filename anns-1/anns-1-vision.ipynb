{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df2cae92-e267-4b37-8690-feabdd1e2f56",
   "metadata": {},
   "source": [
    "# `anns-1`: Deep learning for vision\n",
    "In this lab, we'll explore some common architectures for deep neural networks. You'll need to install [PyTorch](https://pytorch.org/) in your conda environment: `conda install pytorch torchvision -c pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f328c71a-b067-4170-a3f2-2767915e4e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/snastase/miniconda3/envs/neu502b/lib/python3.12/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/snastase/miniconda3/envs/neu502b/lib/python3.12/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
      "  Referenced from: <367D4265-B20F-34BD-94EB-4F3EE47C385B> /Users/snastase/miniconda3/envs/neu502b/lib/python3.12/site-packages/torchvision/image.so\n",
      "  Reason: tried: '/opt/X11/lib/flat_namespace/libjpeg.9.dylib' (no such file), '/Users/snastase/miniconda3/envs/neu502b/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/snastase/miniconda3/envs/neu502b/lib/python3.12/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/snastase/miniconda3/envs/neu502b/lib/python3.12/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/snastase/miniconda3/envs/neu502b/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3102387a-8c7a-4a15-beab-328d8363c406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "# Get device for training (e.g. MacOS 'mps')\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8108cfb6-d941-4bf8-99b9-dd4334f15c81",
   "metadata": {},
   "source": [
    "### Multilayer perceptron\n",
    "In the following exercise, we'll (de)construct a fully-connected feedforward network (i.e. multilayer perceptron; MLP). We'll use the rectified linear unit (ReLU) nonlinearity at hidden and output layers. This network will take as input pixels from MNIST images of handwritten digits and learn to output the correct digit label $0$ to $9$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1702790e-c550-46a2-9597-5f2dae3f2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e096feb5-1b44-4a41-a249-738066afcece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bca3432-7621-4be7-bf40-1b648e336f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# Create a random image and confirm shape\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "print(input_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb69d88-a7f8-4dec-8d93-de0713f3f61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 784])\n"
     ]
    }
   ],
   "source": [
    "# Flatten image and inspect shape\n",
    "flatten = nn.Flatten()\n",
    "flat_image = flatten(input_image)\n",
    "print(flat_image.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a234b7e-8b6c-4a48-823e-ffdb72b0b3a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 20])\n"
     ]
    }
   ],
   "source": [
    "# Pass flattened input image through first layer\n",
    "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
    "hidden1 = layer1(flat_image)\n",
    "print(hidden1.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e2a36a4-1e7f-4608-af82-3b3855eb4eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[ 0.1494, -0.0192, -0.0778, -0.0360,  0.2544,  0.0497,  0.1793,  0.5193,\n",
      "          0.2267,  0.0706,  0.0390,  0.3626,  0.2386, -0.2266, -0.0674, -0.1057,\n",
      "         -0.0605,  0.1466, -0.1666,  0.2050],\n",
      "        [ 0.2361,  0.0026, -0.1876,  0.1045,  0.3015,  0.2611,  0.2626,  0.5896,\n",
      "          0.0936, -0.0553, -0.2797,  0.5407,  0.0157, -0.0383, -0.0080, -0.2019,\n",
      "         -0.2719,  0.1114,  0.0576,  0.1122],\n",
      "        [ 0.0529, -0.2187, -0.2689, -0.2597,  0.3775,  0.0130,  0.1517,  0.7412,\n",
      "          0.0394, -0.1736,  0.0705,  0.5213, -0.0695, -0.2812,  0.1812, -0.1023,\n",
      "          0.0484, -0.2455, -0.3618, -0.0042]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "After ReLU: tensor([[0.1494, 0.0000, 0.0000, 0.0000, 0.2544, 0.0497, 0.1793, 0.5193, 0.2267,\n",
      "         0.0706, 0.0390, 0.3626, 0.2386, 0.0000, 0.0000, 0.0000, 0.0000, 0.1466,\n",
      "         0.0000, 0.2050],\n",
      "        [0.2361, 0.0026, 0.0000, 0.1045, 0.3015, 0.2611, 0.2626, 0.5896, 0.0936,\n",
      "         0.0000, 0.0000, 0.5407, 0.0157, 0.0000, 0.0000, 0.0000, 0.0000, 0.1114,\n",
      "         0.0576, 0.1122],\n",
      "        [0.0529, 0.0000, 0.0000, 0.0000, 0.3775, 0.0130, 0.1517, 0.7412, 0.0394,\n",
      "         0.0000, 0.0705, 0.5213, 0.0000, 0.0000, 0.1812, 0.0000, 0.0484, 0.0000,\n",
      "         0.0000, 0.0000]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Introduce nonlinearity ReLU\n",
    "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
    "hidden1 = nn.ReLU()(hidden1)\n",
    "print(f\"After ReLU: {hidden1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a444a552-d5cc-43c7-9563-3c8c423a92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain these operations into sequence\n",
    "seq_modules = nn.Sequential(\n",
    "    flatten,\n",
    "    layer1,\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(20, 10)\n",
    ")\n",
    "\n",
    "input_image = torch.rand(3, 28, 28)\n",
    "logits = seq_modules(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21dea576-0d0b-4c23-beb7-d536b603f1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits: tensor([[-0.1552,  0.2437, -0.2456,  0.3958, -0.2293, -0.1989, -0.1963,  0.0040,\n",
      "          0.0034, -0.6217],\n",
      "        [-0.0984,  0.1159, -0.1967,  0.3043, -0.0892, -0.2845, -0.1889, -0.0206,\n",
      "         -0.0906, -0.4959],\n",
      "        [-0.2448,  0.1873, -0.1283,  0.2827, -0.1056, -0.2625, -0.1561, -0.1130,\n",
      "         -0.1084, -0.4289]], grad_fn=<AddmmBackward0>)\n",
      "Min: -0.621714\n",
      "Max: 0.395797\n",
      "Softmaxed: tensor([[0.0913, 0.1360, 0.0834, 0.1584, 0.0848, 0.0874, 0.0876, 0.1070, 0.1070,\n",
      "         0.0572],\n",
      "        [0.0985, 0.1220, 0.0893, 0.1473, 0.0994, 0.0818, 0.0900, 0.1064, 0.0992,\n",
      "         0.0662],\n",
      "        [0.0855, 0.1316, 0.0960, 0.1448, 0.0982, 0.0840, 0.0934, 0.0975, 0.0979,\n",
      "         0.0711]], grad_fn=<SoftmaxBackward0>)\n",
      "Min: 0.057246\n",
      "Max: 0.158360\n",
      "tensor([1., 1., 1.], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "# Softmax predictions into probability distribution\n",
    "softmax = nn.Softmax(dim = 1)\n",
    "pred_probab = softmax(logits)\n",
    "print(f\"Logits: {logits}\\nMin: {logits.min():.6f}\\nMax: {logits.max():.6f}\")\n",
    "print(f\"Softmaxed: {pred_probab}\\nMin: {pred_probab.min():.6f}\\n\"\n",
    "      f\"Max: {pred_probab.max():.6f}\\n{pred_probab.sum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a466c77-510c-4df0-8058-b281e1e3181f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6de37cc-4d19-4fe5-b2b2-31dacb6b9fb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "584752ad-2bc4-4fb9-b840-ed7cfb084030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize an example MNIST digit\n",
    "mnist_id = 0\n",
    "plt.matshow(X[mnist_id, 0], cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94da2dea-dde7-44f0-823d-ffdaabf738b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Redefine our model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33778971-6d2d-4220-b992-267f354ba2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81df5516-9ea1-4b66-a89d-568b947bb3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Train the model\n",
    "    model.train()\n",
    "    \n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Clear gradients for this training step\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute gradients via backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Apply gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16be8b49-379c-4eb5-ae1d-b3f22bdab415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    # Test the model\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            \n",
    "            # Compute model predictions on test set\n",
    "            pred = model(X)\n",
    "            \n",
    "            # Compute loss between predicted and actual labels\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            # Number correct for accuracy\n",
    "            correct += (pred.max(axis=1).indices == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test error: \\n Accuracy: {(100*correct):>0.1f}%, mean loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2790151c-33e6-45af-a5dd-274d9b86de9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304243  [    0/60000]\n",
      "loss: 2.294697  [ 6400/60000]\n",
      "loss: 2.293162  [12800/60000]\n",
      "loss: 2.296443  [19200/60000]\n",
      "loss: 2.286513  [25600/60000]\n",
      "loss: 2.276189  [32000/60000]\n",
      "loss: 2.266360  [38400/60000]\n",
      "loss: 2.272683  [44800/60000]\n",
      "loss: 2.265659  [51200/60000]\n",
      "loss: 2.262096  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 43.3%, mean loss: 2.258112 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.259550  [    0/60000]\n",
      "loss: 2.247411  [ 6400/60000]\n",
      "loss: 2.255322  [12800/60000]\n",
      "loss: 2.238732  [19200/60000]\n",
      "loss: 2.240104  [25600/60000]\n",
      "loss: 2.227487  [32000/60000]\n",
      "loss: 2.209081  [38400/60000]\n",
      "loss: 2.231813  [44800/60000]\n",
      "loss: 2.206499  [51200/60000]\n",
      "loss: 2.200574  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 57.1%, mean loss: 2.196673 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.196612  [    0/60000]\n",
      "loss: 2.179090  [ 6400/60000]\n",
      "loss: 2.199589  [12800/60000]\n",
      "loss: 2.153221  [19200/60000]\n",
      "loss: 2.167928  [25600/60000]\n",
      "loss: 2.150754  [32000/60000]\n",
      "loss: 2.117320  [38400/60000]\n",
      "loss: 2.160633  [44800/60000]\n",
      "loss: 2.108896  [51200/60000]\n",
      "loss: 2.096507  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 62.8%, mean loss: 2.091583 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.090655  [    0/60000]\n",
      "loss: 2.062323  [ 6400/60000]\n",
      "loss: 2.101621  [12800/60000]\n",
      "loss: 2.007959  [19200/60000]\n",
      "loss: 2.040678  [25600/60000]\n",
      "loss: 2.015586  [32000/60000]\n",
      "loss: 1.957683  [38400/60000]\n",
      "loss: 2.030788  [44800/60000]\n",
      "loss: 1.939163  [51200/60000]\n",
      "loss: 1.916196  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 66.9%, mean loss: 1.908081 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.909023  [    0/60000]\n",
      "loss: 1.861740  [ 6400/60000]\n",
      "loss: 1.927022  [12800/60000]\n",
      "loss: 1.771174  [19200/60000]\n",
      "loss: 1.817842  [25600/60000]\n",
      "loss: 1.783290  [32000/60000]\n",
      "loss: 1.700793  [38400/60000]\n",
      "loss: 1.815660  [44800/60000]\n",
      "loss: 1.674008  [51200/60000]\n",
      "loss: 1.641745  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 71.4%, mean loss: 1.626207 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Fit the model!\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a858308-1bce-41a3-8c98-36d21abc5063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"6\", Actual: \"5\"\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 480x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize model predictions from the test set\n",
    "classes = np.arange(10)\n",
    "test_id = 8\n",
    "\n",
    "x, y = test_data[test_id][0], test_data[test_id][1]\n",
    "with torch.no_grad():\n",
    "    plt.matshow(x[0], cmap='gray')\n",
    "    x, y = x.to(device), y\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred.max(axis=1).indices], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c7f659-6906-4b8d-b75b-c80595dc3d44",
   "metadata": {},
   "source": [
    "### Convolutional neural network\n",
    "In this excercise, we'll perform the same digit recognition task using a convolutional neural network (CNN). For brevity, the CNN will comprise two convolutional layers followed by max-pooling layers, culminating in a single fully-connected layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b90f50eb-9d4f-4d4f-9048-399992ae5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(         \n",
    "            nn.Conv2d(\n",
    "                in_channels=1,              \n",
    "                out_channels=16,            \n",
    "                kernel_size=4,              \n",
    "                stride=1,                   \n",
    "                padding=2,                  \n",
    "            ),                              \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(kernel_size=2),    \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(         \n",
    "            nn.Conv2d(16, 32, 4, 1, 2),     \n",
    "            nn.ReLU(),                      \n",
    "            nn.MaxPool2d(2),                \n",
    "        )\n",
    "\n",
    "        # Fully connected layer outputs 10 classes\n",
    "        self.out = nn.Linear(32 * 7 * 7, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "\n",
    "        # Flatten the output of conv2 to (batch_size, 32 * 7 * 7)\n",
    "        x = x.view(x.size(0), -1)       \n",
    "        output = self.out(x)\n",
    "        return output\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62cbf5dd-6d0b-45b8-9458-e359cace5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define same loss and optimizer as above\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "18f60dea-266e-4763-9b3b-b3238305f6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.310146  [    0/60000]\n",
      "loss: 2.304029  [ 6400/60000]\n",
      "loss: 2.289904  [12800/60000]\n",
      "loss: 2.280433  [19200/60000]\n",
      "loss: 2.288378  [25600/60000]\n",
      "loss: 2.284853  [32000/60000]\n",
      "loss: 2.264970  [38400/60000]\n",
      "loss: 2.272026  [44800/60000]\n",
      "loss: 2.255702  [51200/60000]\n",
      "loss: 2.237698  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 47.4%, mean loss: 2.244536 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.249099  [    0/60000]\n",
      "loss: 2.233713  [ 6400/60000]\n",
      "loss: 2.232458  [12800/60000]\n",
      "loss: 2.181472  [19200/60000]\n",
      "loss: 2.199187  [25600/60000]\n",
      "loss: 2.185049  [32000/60000]\n",
      "loss: 2.127254  [38400/60000]\n",
      "loss: 2.143290  [44800/60000]\n",
      "loss: 2.079287  [51200/60000]\n",
      "loss: 2.013330  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 68.5%, mean loss: 2.012791 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.024097  [    0/60000]\n",
      "loss: 1.941519  [ 6400/60000]\n",
      "loss: 1.904058  [12800/60000]\n",
      "loss: 1.704564  [19200/60000]\n",
      "loss: 1.637723  [25600/60000]\n",
      "loss: 1.519232  [32000/60000]\n",
      "loss: 1.302003  [38400/60000]\n",
      "loss: 1.331592  [44800/60000]\n",
      "loss: 1.122565  [51200/60000]\n",
      "loss: 0.971178  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 80.3%, mean loss: 0.923816 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.960785  [    0/60000]\n",
      "loss: 0.796248  [ 6400/60000]\n",
      "loss: 0.757576  [12800/60000]\n",
      "loss: 0.684471  [19200/60000]\n",
      "loss: 0.653458  [25600/60000]\n",
      "loss: 0.621045  [32000/60000]\n",
      "loss: 0.513655  [38400/60000]\n",
      "loss: 0.686632  [44800/60000]\n",
      "loss: 0.637053  [51200/60000]\n",
      "loss: 0.575265  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 85.9%, mean loss: 0.523128 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.546529  [    0/60000]\n",
      "loss: 0.495667  [ 6400/60000]\n",
      "loss: 0.424087  [12800/60000]\n",
      "loss: 0.478624  [19200/60000]\n",
      "loss: 0.440021  [25600/60000]\n",
      "loss: 0.454728  [32000/60000]\n",
      "loss: 0.345361  [38400/60000]\n",
      "loss: 0.534265  [44800/60000]\n",
      "loss: 0.505824  [51200/60000]\n",
      "loss: 0.494649  [57600/60000]\n",
      "Test error: \n",
      " Accuracy: 88.2%, mean loss: 0.416944 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Fit the model!\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
